from __future__ import annotations

import pytest
import numpy as np
import pandas as pd

from .categorical import CategoricalNB
from sklearn.naive_bayes import CategoricalNB as sk_naive_bayes
from sklearn.preprocessing import OneHotEncoder 

"""
====================================================
============== Auxillary functions =================
====================================================
"""
def X():
    yield from [
        {'age': "senior", 'income': "fair"},
        {'age': "junior", 'income': "poor"},
        {'age': "young", 'income': "fair"},
        {'age': "senior", 'income': "fair"},
        {'age': "junior", 'income': "fair"},
    ]

def Y():
    yield from ["Yes","Yes","No","No","Yes"]

def incremental_model(alpha = 1.0):
    model = CategoricalNB(alpha = alpha)

    for x,y in zip(X(),Y()):
        model.learn_one(x, y) 
    return model

def batch_model(alpha = 1.0):
    model = CategoricalNB(alpha = alpha)

    return model.learn_many(
        pd.DataFrame([x for x in X()]),
        pd.Series([y for y in Y()])
        )

def n_category_count(model):
        return sum([len(v) for _,v in model.category_counts.items()])

"""
====================================================
==================== Test Cases ====================
====================================================
"""

@pytest.mark.parametrize(
        "model",
        [
            pytest.param(
                incremental_model(alpha = alpha),
                 id=f"alpha - {alpha}"
            )
            for alpha in range(1,4)
        ]
)
def test_learn_one(model):

    labels = [str(label).lower() for label in np.unique(y for y in Y())]

    # non-negative validation for class counts
    for label in labels:
        assert model.class_counts[label] >= 0

@pytest.mark.parametrize(
        "model",
        [
            pytest.param(
                incremental_model(alpha = alpha),
                 id=f"alpha - {alpha}"
            )
            for alpha in range(1,4)
        ]
)
def test_p_feature_given_class(model):

    values = []
    labels = [str(label).lower() for label in np.unique([y for y in Y()])]
    # conditional probability
    for label in labels:
        values.append(model.p_feature_given_class(feature='age', category = 'young', label =label))
        values.append(model.p_feature_given_class(feature='income', category = 'fair', label =label))

    # constraint check on probability values
    for p in values:
        assert p <= 1 and p > 0

@pytest.mark.parametrize(
        "model",
        [
            pytest.param(
                incremental_model(alpha = alpha),
                 id=f"alpha - {alpha}"
            )
            for alpha in range(1,4)
            
        ]
)
def test_p_class(model):

    # sum of priors must between 0 and 1
    assert model.p_class('yes') + model.p_class('no') <=1 and model.p_class('yes') + model.p_class('no') > 0

@pytest.mark.parametrize(
        "model",
        [
            pytest.param(
                incremental_model(alpha = alpha),
                id=f"alpha - {alpha}"
            )
            for alpha in range(1,4)
        ]
)
def test_joint_log_likelihood(model):
    labels = [str(label).lower() for label in np.unique([y for y in Y()])]
    values = [
        model.joint_log_likelihood(sample)[label]
        for label in labels for sample in X()
    ]

    # constraint check on log prob values
    for val in values:
        assert val < 0

@pytest.mark.parametrize(
        "model",
        [
            pytest.param(
                batch_model(alpha = alpha),
                 id=f"alpha - {alpha}"
            )
            for alpha in range (1,4)
        ]
)
def test_learn_many(model):
    labels = [str(label).lower() for label in np.unique([y for y in Y()])]

    # non-negative validation for class counts
    for label in labels:
        assert model.class_counts[label] >= 0

@pytest.mark.parametrize(
        "model, batch_model",
        [
            pytest.param(
                incremental_model(alpha = alpha),
                batch_model(alpha = alpha),
                id=f"alpha - {alpha}"
            ) for alpha in range(1,4)
        ]
)
def test_one_vs_many(model,batch_model):
    """
    This test evaluates results generated by incremental and batch approach on the same dataset.
    """

    labels = [str(label).lower() for label in np.unique([y for y in Y()])]
    features = [k for k in model.feature_counts.keys()]

    # Evaluating state of both models with each other
    for label in labels:
        # Phase1: non-negative validation for class counts
        assert model.class_counts[label] >= 0
        assert batch_model.class_counts[label] >= 0
    
        # Phase2: Assert class count for both models
        assert model.class_counts[label] == batch_model.class_counts[label]

        # Phase3: Assert feature count for both models
        for feature in features:
            assert model.feature_counts[feature][label] == batch_model.feature_counts[feature][label]

    # Phase4:  Assert distinct category count for both models
    assert n_category_count(model) == n_category_count(batch_model)
    
@pytest.mark.parametrize(
        "rv_model, sk_model, encoder" ,
        [
            pytest.param(
                CategoricalNB(alpha = alpha),
               sk_naive_bayes(alpha = alpha),
                OneHotEncoder(),
                id=f"alpha - {alpha}"
            ) for alpha in range(1,4)
        ]
)
def test_river_vs_sklearn(rv_model, sk_model, encoder):
    
    x = pd.DataFrame([x for x in X()])
    y = pd.Series([y for y in Y()])
    encodedX = encoder.fit_transform(x).toarray()

    rv_model.learn_many(x,y)
    sk_model.fit(encodedX,y)

    # Assert feature count of both models
    assert n_category_count(rv_model) == sk_model.n_features_in_

    # Assert class count of both models
    assert sum(rv_model.class_counts.values()) == int(sum(sk_model.class_count_))

